#the question was to predict that the market will increase by 50 points in the next 13 minutes

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

# Step 1: Load and preprocess the market data
# Assume df contains features relevant to market prediction and a column indicating whether the market increased by 50 points in 13 minutes
# Example DataFrame creation for demonstration
np.random.seed(42)
df = pd.DataFrame({
    'feature1': np.random.rand(1000),
    'feature2': np.random.rand(1000),
    'feature3': np.random.rand(1000),
    'price_change': np.random.rand(1000) * 100  # Example of price change data
})

# Creating the target: whether the market increased by 50 points in 13 minutes (binary classification)
# Example target definition; in practice, calculate based on actual market data.
df['target'] = (df['price_change'] >= 50).astype(int)

# Split into features and targets
X = df[['feature1', 'feature2', 'feature3']]  # Replace with your actual features
y = df['target']  # Binary target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 2: Build a Binary Classification Neural Network Model
input_layer = Input(shape=(X_train_scaled.shape[1],))

# Neural network layers
x = Dense(64, activation='relu')(input_layer)
x = Dense(32, activation='relu')(x)

# Output layer: binary classification (0 or 1)
output = Dense(1, activation='sigmoid')(x)  # Sigmoid activation for binary classification

# Define the model
model = Model(inputs=input_layer, outputs=output)

# Step 3: Compile and Train the Model
model.compile(optimizer=Adam(learning_rate=0.001), loss ='binary_crossentropy', lossmetrics =['accuracy']) # Binary classification

# Train the model
history = model.fit(X_train_scaled, y_train,  validation_data=(X_test_scaled, y_test), epochs=50, batch_size=32)

# Step 4: Evaluate the Model
y_pred = model.predict(X_test_scaled)
y_pred_classes = (y_pred > 0.5).astype(int)  # Threshold of 0.5 for binary classification

# Classification report
print(classification_report(y_test, y_pred_classes))
print(f'Accuracy: {accuracy_score(y_test, y_pred_classes)}')

#2
#Write a Python program to find the factorial of a number using recursion.

def factorial(n):
    if n == 1:
        return 1
    else:
        return n * factorial(n-1)

#3
#Write a Python program that reads a file, counts the number of lines, and prints the result.

def count_lines(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()
        print("Number of lines:", len(lines))

#4
#Write a single line Python code to print all the prime numbers between 1 and 200.

print([x for x in range(2, 201) if all(x % i != 0 for i in range(2, int(x**0.5) + 1))])

#5
#Python decorators, and how are they used in AI/ML?
def timer(func):
    import time
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        print(f"Time taken: {end_time - start_time} seconds")
        return result
    return wrapper

@timer
def train_model(data):
    # Simulating model training
    return sum(data)

train_model([i for i in range(100000)])

#6
#gradient descent in Python?
def gradient_descent(x, y, learning_rate=0.01, iterations=1000):
    m, b = 0, 0  # Initial guess for the slope and intercept
    n = len(x)   # Number of samples

    for _ in range(iterations):
        y_pred = m * x + b  # Prediction
        dm = (-2/n) * sum(x * (y - y_pred))  # Gradient w.r.t m
        db = (-2/n) * sum(y - y_pred)        # Gradient w.r.t b
        m -= learning_rate * dm  # Update m
        b -= learning_rate * db  # Update b

    return m, b

